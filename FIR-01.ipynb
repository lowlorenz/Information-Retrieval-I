{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - _Foundations of Information Retrieval '20/'21_\n",
    "\n",
    "This assignment is divided in 3 parts, which have to be delivered all together before 04/10/2020 (strictly - no extensions will be granted!), via Canvas. Delivery of the assignment solutions is mandatory.\n",
    "\n",
    "We will use [ElasticSearch](https://www.elastic.co/) as search engine, as it provides state-of-the-art tools to implement your own engine, and let you focus on methodological aspects of search implementation and optimization.\n",
    "\n",
    "The assignment is about text-based Information Retrieval and it is structured in three parts:\n",
    "1. IR performance evaluation (implementation of performance metrics)\n",
    "2. Setting up a search engine, pre-processing and indexing using ElasticSearch (Indexing, Analyzers)\n",
    "3. Implementation and optimization of a model of search using ElasticSearch (Similarity measures)\n",
    "\n",
    "\n",
    "This assignment file contains exercises, marked with the section title __Exercise 01.(x)__, which are evaluated, and other sections that contain support code which you should use as it is. Write your answers between the comments `BEGIN ANSWER` and `END ANSWER`.\n",
    "Try to complete the solutions for all the exercise sections. \n",
    "\n",
    "_Note:_ we leave the comment `#THIS IS GRADED!` in the sections that will be considered for evauation and grading.\n",
    "\n",
    "\n",
    "### Initial preparation (self-study)\n",
    "For the It is good to acquire basic knowledge of Python (or refresh it a bit).\n",
    "For the second and third part of the assignment, please study yourself the [Getting Started guide](https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html)\" of ElasicSearch and get acquainted with the framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 01 - Performance evaluation\n",
    "\n",
    "\n",
    "### Background information and reading\n",
    "To solve the exercises in Part 01, study the slides of Lecture 01 (available on Canvas) and the reference book chapter (Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Introduction to Information Retrieval, [Chapter 8, Evaluation in information retrieval](http://nlp.stanford.edu/IR-book/pdf/08eval.pdf), Cambridge University Press. 2008)\n",
    "\n",
    "### Basic concepts\n",
    "Suppose the set of relevant documents (the document identifiers - _doc-IDs_) is called `relevant`, then we might define those as follows (in Python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant = set([2, 3, 5, 8, 13, 21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect run would retrieve exactly these 6 documents in any order. Now, suppose the list of retrieved documents (the document identifiers - _doc-IDs_) is called `retrieved`, and contains the following _doc-IDs_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved = [4, 2, 18, 16, 8, 46, 32, 22, 47, 39, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest evaluation measures we can think of is the _Success at rank 1_. The measure answers the question: Was the first document retrieved a relevant document? _Success at rank 1_ returns 1 if the first document is relevant, and 0 otherwise. A possible implementation is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def success_at_1 (relevant, retrieved):\n",
    "    if len(retrieved) > 0 and retrieved[0] in relevant:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "success_at_1(relevant, retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first retrieved documentid is 4 which is not in the set of relevant documents, so the score is 0.\n",
    "\n",
    "Note how easy it is to check if an item occurs in a Python set or list by using the keyword: `in`. Similarly, you can loop over all items in a set of list with: \n",
    "`for doc in retrieved:`, \n",
    "where doc will refer to each item in the set or list. \n",
    "\n",
    "Be sure to use the internet to sharpen your knowledge about Python constructs, for instance on [Python list slicing](https://duckduckgo.com/?q=python+list+slicing). Also note that the code above checks if at least one document is retrieved to avoid an index out of bounds exception (i.e. we avoid to access an empty vector).\n",
    "\n",
    "> __Suggestion: to be sure of the correctness of the implementations of the performance metrics you are requested, you can compute their values manually and compare them with those of your functions. This is important, as you will use these metrics for later exercises and to compare different models.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01.A: _Success at k_\n",
    "The measure _Success at k_ returns 1 if a relevant document is among the first _k_ documents retrieved and zero otherwise. Implement _Success at 5_ below.\n",
    "\n",
    "> Success at _k_ measures are well-suited in cases where there is typically only one relevant document (or retrieving one relevant document is enough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "\n",
    "def success_at_5(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    for k in retrieved[:5]:\n",
    "        if k in relevant:\n",
    "            return 1        \n",
    "    return 0\n",
    "    # END ANSWER\n",
    "    \n",
    "success_at_5(relevant, retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly implement success at rank 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def success_at_10(relevant, retrieved):\n",
    "    # BEGIN ANSWER   \n",
    "    for k in retrieved[:10]:\n",
    "        if k in relevant:\n",
    "            return 1   \n",
    "    return 0\n",
    "    # END ANSWER\n",
    "    \n",
    "success_at_10(relevant, retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01.B: _Precision, Recall and F-measure_\n",
    "Implement _Precision_ using Formula 8.1 from [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book).\n",
    "\n",
    "_Hint:_ one can count the number of documents in a list by using the built-in Python function [len()](https://docs.python.org/3/library/functions.html#len) (e.g. `len(retrieved)` for the number of retrieved documents). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2727272727272727"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def precision(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    if not retrieved:\n",
    "        return 1\n",
    "    \n",
    "    relevant_and_retrieved = [k for k in retrieved if k in relevant]\n",
    "    return len(relevant_and_retrieved) / len(retrieved)\n",
    "    # END ANSWER\n",
    "    \n",
    "precision(relevant, retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement _Recall_ using Formula 8.2 from [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def recall(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    if not relevant:\n",
    "        return 1\n",
    "    \n",
    "    relevant_and_retrieved = [k for k in retrieved if k in relevant]\n",
    "    return len(relevant_and_retrieved) / len(relevant)\n",
    "    # END ANSWER\n",
    "    \n",
    "recall(relevant, retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The balanced F measure (_F_ with β=1) is defined as the harmonic mean of precision and\n",
    "recall. Implement _F_ using Formula 8.6 from [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book).\n",
    "\n",
    "> Tip: reuse your implementations of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3529411764705882"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def f_measure(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    P = precision(relevant, retrieved)\n",
    "    R = recall(relevant, retrieved)\n",
    "    return 2*P*R/(P+R)\n",
    "    # END ANSWER\n",
    "    \n",
    "f_measure(relevant, retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01.C: _Precision at rank k_ and  _R-Precision_\n",
    "\n",
    "Precision, Recall and F are _set_-based measures and suited for unranked lists of documents. If our search system returns a ranked _list_ of results, we can measure precision for several cut-off levels _k_ in the ranked list, i.e. we evaluate the relevance of the TOP-_k_ retrieved documents (see lecture slides and the related book chapter). \n",
    "We did this before with the _Success at rank 5_ measure for _k_=5.\n",
    "\n",
    "Implement below the function `precision_at_k()` that measures the precision at rank _k_\n",
    "\n",
    "> Interesting fact: For _k_=1, the _Precision at rank 1_ would be the samen as _Success at rank 1_ (why?) - Because it must be either 1 out of 1 right or 0 out of 1 correct so. Therefore the precision must be 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def precision_at_k(relevant, retrieved, k):\n",
    "    # BEGIN ANSWER\n",
    "    return precision(relevant, retrieved[:k])\n",
    "    # END ANSWER\n",
    "    \n",
    "precision_at_k(relevant, retrieved, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement R-Precision (function `r_precision()`) as defined on Page 161 of [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def r_precision(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    k = len(relevant)\n",
    "    return precision(relevant, retrieved[:k])\n",
    "    # END ANSWER\n",
    "    \n",
    "r_precision(relevant, retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01.D:  Interpolated precision at _recall_ X\n",
    "\n",
    "Another way to address ranked retrieval is to measure precision for several _recall_ levels _X_.\n",
    "\n",
    "Implement the function `interpolated_precision_at_recall_X()` that measures the interpolated precision at recall level _X_ as defined by Formula 8.7 of [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book).\n",
    "\n",
    "> Tip: calculate for each rank the recall. If the recall is greater than or equal to X, \n",
    "> calculate the precision. Keep the highest (maximum) precision of those to be returned at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def interpolated_precision_at_recall_X (relevant, retrieved, X):\n",
    "    # BEGIN ANSWER\n",
    "    # The interpolated precision at recall X is undefined where the max recall for the retrieved set does not reach X.\n",
    "    if recall(relevant, retrieved) < X:\n",
    "        return 0\n",
    "    \n",
    "    P = 0    \n",
    "    # Loop through each rank.\n",
    "    for k, _ in enumerate(retrieved):\n",
    "        if recall(relevant, retrieved[:k]) >= X:\n",
    "            P = max(P, precision_at_k(relevant, retrieved, k))\n",
    "    \n",
    "    return P\n",
    "    # END ANSWER\n",
    "    \n",
    "interpolated_precision_at_recall_X(relevant, retrieved, X=0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01.E:  _Average Precision_\n",
    "\n",
    "For a single information need, _Average Precision_ is the average of the precision value obtained for the set of top k documents existing after each relevant document is retrieved (see [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book), Pages 159 and 160). Implement _Average Precision_ for a single information need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.075"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def average_precision(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    # Initialise list of precisions with a zero for each relevant document.\n",
    "    P = [0] * len(relevant)\n",
    "    \n",
    "    for i, doc in enumerate(relevant):\n",
    "        # If a relevant document is not retrieved, the precision value is taken to be zero. \n",
    "        if doc not in retrieved:\n",
    "            P[i] = 0\n",
    "        else:\n",
    "            # Find the precision for the top k documents when doc is retrieved.\n",
    "            k = retrieved.index(doc)\n",
    "            P[i] = precision_at_k(relevant, retrieved, k)\n",
    "    \n",
    "    # Return the average precision\n",
    "    return sum(P)/len(P)\n",
    "    # END ANSWER\n",
    "\n",
    "average_precision(relevant, retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures in TREC \n",
    "\n",
    "The relevance judgments are provided by TREC in so-called _\"qrels\"_ files that look as follows:\n",
    "\n",
    "    1000 Q0 1341 1\n",
    "    1000 Q0 1231 0\n",
    "    1001 Q0 12332 1\n",
    "     ...\n",
    "\n",
    "The first column is the query identifier, while the second column is the query number within that topic (it is currently unused and should always be Q0). The third column is the document identifier that was examined by the judges. The fourth column is the relevance of the document (_1_ means the document was relevant and _0_ means the document was not relevant).\n",
    "\n",
    "Below we provide some Python code that reads the _qrels_ and the _run_. The qrels will be put in the Python dictionary `all_relevant`. A [Python dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) provides quick lookup of a set of values given a key. We will use the `query_id` as a key, and a [set](https://docs.python.org/3/tutorial/datastructures.html#sets) of relevant document identifiers. For the partial qrels file above, `all_relevant` would look as follows:\n",
    "\n",
    "    {\n",
    "        \"1000\": set([\"1341\", \"1231\"]),\n",
    "        \"1001\": set([\"12332\"])\n",
    "    }\n",
    "    \n",
    "We will use a dictionary called `all_retrieved` with `query_id` as key, and as value a [Python list](https://docs.python.org/3/tutorial/introduction.html#lists) of document identifiers retrieved by the IR system:\n",
    "\n",
    "    {\n",
    "        \"1000\": [\"1341\", \"12346, \"2345\"],\n",
    "        \"1001\": [..., ..., ...],\n",
    "        ...\n",
    "    }\n",
    "\n",
    "Note that, with this data structure, for each `query_id` we can easily access the list of retrieved and relevant documents, and compute the performance metrics. We can then average these measures over all the queries to compute the mean performance of the IR system on the given retrieval task.\n",
    "\n",
    "Please examine the code below, and make sure you understand every line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_qrels_file(qrels_file):  # reads the content of he qrels file\n",
    "    trec_relevant = dict()  # query_id -> set([docid1, docid2, ...])\n",
    "    with open(qrels_file, 'r') as qrels:\n",
    "        for line in qrels:\n",
    "            (qid, q0, doc_id, rel) = line.strip().split()\n",
    "            if qid not in trec_relevant:\n",
    "                trec_relevant[qid] = set()\n",
    "            if (rel == \"1\"):\n",
    "                trec_relevant[qid].add(doc_id)\n",
    "    return trec_relevant\n",
    "\n",
    "def read_run_file(run_file):  \n",
    "    # read the content of the run file produced by our IR system \n",
    "    # (in the following exercises you will create your own run_files)\n",
    "    trec_retrieved = dict()  # query_id -> [docid1, docid2, ...]\n",
    "    with open(run_file, 'r') as run:\n",
    "        for line in run:\n",
    "            (qid, q0, doc_id, rank, score, tag) = line.strip().split()\n",
    "            if qid not in trec_retrieved:\n",
    "                trec_retrieved[qid] = []\n",
    "            trec_retrieved[qid].append(doc_id) \n",
    "    return trec_retrieved\n",
    "    \n",
    "\n",
    "def read_eval_files(qrels_file, run_file):\n",
    "    return read_qrels_file(qrels_file), read_run_file(run_file)\n",
    "\n",
    "(all_relevant, all_retrieved) = read_eval_files('data/training-qrels.txt', 'data/baselineTREC.run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 01.F: _number of queries_ and _number of retrieved documents per query_\n",
    " \n",
    "Write the Python code that counts the number of queries in the file `baseline.run` and print the value (use the result from the cell above). \n",
    "\n",
    "_Hint:_ print the structure and content of the `all_retrieved` and `all_relevant` data structures to understand them better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "# BEGIN ANSWER\n",
    "\n",
    "# baselineTREC.run is read into the dict all_retrieved. By finding the length, we get the number of keys (queries)\n",
    "num_queries = len(all_retrieved)\n",
    "print(num_queries)\n",
    "\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code that counts, for each query in your baseline run, the number of documents that were retrieved for that query (use `print()` to print the result for each `query_id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  1   # Documents Retrieved:  10\n",
      "Query:  2   # Documents Retrieved:  10\n",
      "Query:  3   # Documents Retrieved:  10\n",
      "Query:  4   # Documents Retrieved:  10\n",
      "Query:  5   # Documents Retrieved:  10\n",
      "Query:  6   # Documents Retrieved:  10\n",
      "Query:  7   # Documents Retrieved:  10\n",
      "Query:  8   # Documents Retrieved:  10\n",
      "Query:  9   # Documents Retrieved:  10\n",
      "Query:  10   # Documents Retrieved:  10\n",
      "Query:  11   # Documents Retrieved:  10\n",
      "Query:  12   # Documents Retrieved:  10\n",
      "Query:  13   # Documents Retrieved:  10\n",
      "Query:  14   # Documents Retrieved:  10\n",
      "Query:  15   # Documents Retrieved:  10\n",
      "Query:  16   # Documents Retrieved:  10\n",
      "Query:  17   # Documents Retrieved:  10\n",
      "Query:  18   # Documents Retrieved:  10\n",
      "Query:  19   # Documents Retrieved:  10\n",
      "Query:  20   # Documents Retrieved:  10\n",
      "Query:  21   # Documents Retrieved:  10\n",
      "Query:  22   # Documents Retrieved:  10\n",
      "Query:  23   # Documents Retrieved:  10\n",
      "Query:  24   # Documents Retrieved:  10\n",
      "Query:  25   # Documents Retrieved:  10\n",
      "Query:  26   # Documents Retrieved:  10\n",
      "Query:  27   # Documents Retrieved:  10\n",
      "Query:  28   # Documents Retrieved:  10\n",
      "Query:  29   # Documents Retrieved:  10\n",
      "Query:  30   # Documents Retrieved:  10\n",
      "Query:  31   # Documents Retrieved:  10\n",
      "Query:  32   # Documents Retrieved:  10\n",
      "Query:  33   # Documents Retrieved:  10\n",
      "Query:  34   # Documents Retrieved:  10\n",
      "Query:  35   # Documents Retrieved:  10\n",
      "Query:  36   # Documents Retrieved:  10\n",
      "Query:  37   # Documents Retrieved:  10\n",
      "Query:  38   # Documents Retrieved:  10\n",
      "Query:  39   # Documents Retrieved:  10\n",
      "Query:  40   # Documents Retrieved:  10\n",
      "Query:  41   # Documents Retrieved:  10\n",
      "Query:  42   # Documents Retrieved:  10\n",
      "Query:  43   # Documents Retrieved:  10\n",
      "Query:  44   # Documents Retrieved:  10\n",
      "Query:  45   # Documents Retrieved:  10\n",
      "Query:  46   # Documents Retrieved:  10\n",
      "Query:  47   # Documents Retrieved:  10\n",
      "Query:  48   # Documents Retrieved:  10\n",
      "Query:  49   # Documents Retrieved:  10\n",
      "Query:  50   # Documents Retrieved:  10\n"
     ]
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "# BEGIN ANSWER\n",
    "for query in all_retrieved:\n",
    "    num_documents = len(all_retrieved[query])\n",
    "    print(\"Query: \", query, \"  # Documents Retrieved: \", num_documents)\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01.G: _mean average precision_\n",
    "Using the `average_precision()` function you implemented above, write the code to compute the _Mean Average Precision_ for the `baseline.run` results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mean AP: ', 0.05616925010473398)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def mean_average_precision(all_relevant, all_retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    \n",
    "    count = len(all_retrieved)\n",
    "        \n",
    "    precision_per_query = [average_precision(all_relevant[query], all_retrieved[query])  for query in all_retrieved]\n",
    "    total = sum(precision_per_query)\n",
    "    \n",
    "    # END ANSWER\n",
    "    return \"mean AP: \", total / count\n",
    "\n",
    "mean_average_precision(all_relevant, all_retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREC evaluation\n",
    "\n",
    "Below you find a function that take `all_relevant` and `all_retrieved` to compute the mean result. It computes the mean value over all queries. The function `mean_metric()`'s first function argument, `measure`, is a special argument: it is a function too! The `mean_metric` function sums the total score for the particular measure and divides it by the number of queries. It computes the average measures over all the queries' results.\n",
    "\n",
    "_This part will be reused later to compare the results of different models._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mean r_precision', 0.0769047619047619)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_metric(measure, all_relevant, all_retrieved):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for qid in all_relevant:\n",
    "        relevant  = all_relevant[qid]\n",
    "        retrieved = all_retrieved.get(qid, [])\n",
    "        value = measure(relevant, retrieved)\n",
    "        total += value\n",
    "        count += 1\n",
    "    return \"mean \" + measure.__name__, total / count\n",
    "\n",
    "# Example of use of the mean_metric function: computing the average r_precision\n",
    "mean_metric(r_precision, all_relevant, all_retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC overview of the results\n",
    "The following two functions use your implementation of the metrics to create an evaluation overview of the TREC benchmark data. Give a look at the numbers and make you own interpretations of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data/baselineTREC.run\n",
      "mean success_at_1              0.16\n",
      "mean success_at_5              0.24\n",
      "mean success_at_10             0.3\n",
      "mean r_precision               0.0769\n",
      "mean precision_at_1            0.16\n",
      "mean precision_at_5            0.08\n",
      "mean precision_at_10           0.052\n",
      "mean precision_at_50           0.052\n",
      "mean precision_at_100          0.052\n",
      "mean precision_at_recall_00    1.0\n",
      "mean precision_at_recall_01    0.1747\n",
      "mean precision_at_recall_02    0.1047\n",
      "mean precision_at_recall_03    0.08028\n",
      "mean precision_at_recall_04    0.08028\n",
      "mean precision_at_recall_05    0.08028\n",
      "mean precision_at_recall_06    0.0425\n",
      "mean precision_at_recall_07    0.02917\n",
      "mean precision_at_recall_08    0.02917\n",
      "mean precision_at_recall_09    0.02917\n",
      "mean precision_at_recall_10    0.02917\n",
      "mean average_precision         0.05617\n"
     ]
    }
   ],
   "source": [
    "def trec_eval(qrels_file, run_file):\n",
    "\n",
    "    def precision_at_1(rel, ret): return precision_at_k(rel, ret, k=1)\n",
    "    def precision_at_5(rel, ret): return precision_at_k(rel, ret, k=5)\n",
    "    def precision_at_10(rel, ret): return precision_at_k(rel, ret, k=10)\n",
    "    def precision_at_50(rel, ret): return precision_at_k(rel, ret, k=50)\n",
    "    def precision_at_100(rel, ret): return precision_at_k(rel, ret, k=100)\n",
    "    def precision_at_recall_00(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.0)\n",
    "    def precision_at_recall_01(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.1)\n",
    "    def precision_at_recall_02(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.2)\n",
    "    def precision_at_recall_03(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.3)\n",
    "    def precision_at_recall_04(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.4)\n",
    "    def precision_at_recall_05(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.5)\n",
    "    def precision_at_recall_06(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.6)\n",
    "    def precision_at_recall_07(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.7)\n",
    "    def precision_at_recall_08(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.8)\n",
    "    def precision_at_recall_09(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.9)\n",
    "    def precision_at_recall_10(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=1.0)\n",
    "\n",
    "    (all_relevant, all_retrieved) = read_eval_files(qrels_file, run_file)\n",
    "    \n",
    "    unknown_qids = set(all_retrieved.keys()).difference(all_relevant.keys())\n",
    "    if len(unknown_qids) > 0:\n",
    "        raise ValueError(\"Unknown qids in run: {}\".format(sorted(list(unknown_qids))))\n",
    "\n",
    "    metrics = [success_at_1,\n",
    "               success_at_5,\n",
    "               success_at_10,\n",
    "               r_precision,\n",
    "               precision_at_1,\n",
    "               precision_at_5,\n",
    "               precision_at_10,\n",
    "               precision_at_50,\n",
    "               precision_at_100,\n",
    "               precision_at_recall_00,\n",
    "               precision_at_recall_01,\n",
    "               precision_at_recall_02,\n",
    "               precision_at_recall_03,\n",
    "               precision_at_recall_04,\n",
    "               precision_at_recall_05,\n",
    "               precision_at_recall_06,\n",
    "               precision_at_recall_07,\n",
    "               precision_at_recall_08,\n",
    "               precision_at_recall_09,\n",
    "               precision_at_recall_10,\n",
    "               average_precision]\n",
    "\n",
    "    return [mean_metric(metric, all_relevant, all_retrieved) for metric in metrics]\n",
    "\n",
    "\n",
    "def print_trec_eval(qrels_file, run_file):\n",
    "    results = trec_eval(qrels_file, run_file)\n",
    "    print(\"Results for {}\".format(run_file))\n",
    "    for (metric, score) in results:\n",
    "        print(\"{:<30} {:.4}\".format(metric, score))\n",
    "\n",
    "print_trec_eval('data/training-qrels.txt', 'data/baselineTREC.run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 01.H: _significance testing_\n",
    "\n",
    "Testing the statistical significance of differences in the results of different IR systems is important (see slides and course book - Section 8.8). One of the basic tests one can perform is the two-tailed [sign test](https://en.wikipedia.org/wiki/Sign_test).\n",
    "\n",
    "\n",
    "For this exercise, we use the run files obtained by  [Hiemstra and Aly](https://djoerdhiemstra.com/wp-content/uploads/trec2014mirex-draft.pdf) for TREC 2014. The `utbase.run` file was generated usinf Language Modeling, while `utexact.run` was generated using an IR system based on mathing the exact query string, abd ranking the documents by  the number of exact matches found. The exact run improves the _Precision at 5_ to 0.456 (compared to 0.440 for the baseline run).  \n",
    "\n",
    "Implement the code to perform the _sign test_ of statistical significance.\n",
    "_Hint:_ for each sign, compute the number of queries that increase/descrease performance (called `better, worse` in the code below). How would you use these values to compute the _p_ value of the two-tailed sign test? Is the difference between _utbase_ and _utexact_ significant?\n",
    "    \n",
    "Answer: Conduct a binomial test where `better` is the number of successes, `worse` is the number of failures, and the null hypothesis assumes a binomial distribution with p = 0.5. \n",
    "Since the performance of the second method is better for 9 queries and also worse for 9 queries, then we get a p-value of 1.0 and fail to reject the null hypothesis. i.e. the difference between _utbase_ and _utexact_ is not statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def sign_test_values(measure, qrels_file, run_file_1, run_file_2):\n",
    "    all_relevant = read_qrels_file(qrels_file)\n",
    "    all_retrieved_1 = read_run_file(run_file_1)\n",
    "    all_retrieved_2 = read_run_file(run_file_2)\n",
    "    better = 0\n",
    "    worse  = 0\n",
    "    # BEGIN ANSWER\n",
    "    \n",
    "    for query in all_retrieved_1:\n",
    "        performance_1 = measure(all_relevant[query], all_retrieved_1[query])\n",
    "        performance_2 = measure(all_relevant[query], all_retrieved_2[query])\n",
    "        \n",
    "        if performance_2 > performance_1:\n",
    "            better += 1\n",
    "        # Exclude queries with no performance difference between the two methods.\n",
    "        elif performance_2 < performance_1:\n",
    "            worse += 1\n",
    "    \n",
    "    # END ANSWER\n",
    "    return(better, worse)\n",
    "    \n",
    "def precision_at_rank_5(rel, ret):\n",
    "    return precision_at_k(rel, ret, k=5)\n",
    "\n",
    "sign_test_values(precision_at_rank_5, 'data/trec.qrels', 'data/utbase.run', 'data/utexact.run')\n",
    "\n",
    "# from scipy.stats import binom_test\n",
    "# w = sign_test_values(precision_at_rank_5, 'data/trec.qrels', 'data/utbase.run', 'data/utexact.run')\n",
    "# binom_test(w) # Accept the default arguments for the function\n",
    "### Returns a p-value of 1 > 0.05, thus we fail to reject the null. i.e. there is no difference in performance between the two methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 02 - Indexing and querying with ElasticSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation: Getting started with Elasticsearch\n",
    "\n",
    "We strongly advice you to go through the \"Elasticsearch, [reference guide](https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html)\", and work on the tutorials. The following parts of the assignment will be based on ElasticSearch.\n",
    "\n",
    "You can skip the section on [Installation](https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html), as we provide it already installed in the Virtual Machine.\n",
    "\n",
    "> If you want (disclaimer: we do __not__ give help with this!), you can \n",
    "> follow the [Installation](https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html) to run Elasticsearch on your laptop without VM. But beware, your system will now be different from the \n",
    "> ones of your colleagues and they might not be able to help you if \n",
    "> you have problems that are specific to your system, your operating\n",
    "> system, or your Elasticsearch version.\n",
    "\n",
    "### Starting/Stopping ElasticSearch\n",
    "To start ElasticSearch on the virtual machine, you can type `sudo service elasticsearch start` in a Terminal.\n",
    "To stop the ElasticSearch server, instead, you can type `sudo service elasticsearch stop`. Refer at the [the official guide](https://www.elastic.co/guide/en/elasticsearch/reference/current/deb.html#deb-running-init), for more information.\n",
    "\n",
    "### The REST API\n",
    "\n",
    "Elasticsearch runs its own server that can be accessed by a regular web browser as the client, for instance by opening this link in your browser: http://localhost:9200. \n",
    "\n",
    "Elasticsearch will respond with something like:\n",
    "\n",
    "    {\n",
    "        \"name\" : \"fir-machine\",\n",
    "        \"cluster_name\" : \"elasticsearch\",\n",
    "        \"cluster_uuid\" : \"w7SBVo1ESVivMApbLIqRvA\",\n",
    "        \"version\" : {\n",
    "            \"number\" : \"7.9.0\",\n",
    "            \"build_flavor\" : \"default\",\n",
    "            \"build_type\" : \"deb\",\n",
    "            \"build_hash\" : \"a479a2a7fce0389512d6a9361301708b92dff667\",\n",
    "            \"build_date\" : \"2020-08-11T21:36:48.204330Z\",\n",
    "            \"build_snapshot\" : false,\n",
    "            \"lucene_version\" : \"8.6.0\",\n",
    "            \"minimum_wire_compatibility_version\" : \"6.8.0\",\n",
    "            \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n",
    "        },\n",
    "        \"tagline\" : \"You Know, for Search\"\n",
    "    }\n",
    "\n",
    "\n",
    "If you see this, then your Elasticsearch node is up and running. The RESTful API uses simple text or JSON over HTTP. \n",
    "\n",
    "> REST, API, JSON, HTTP, that's a lot of abbreviations! It is good to\n",
    "> be familiar with the terminology. Let us explain: The Elasticsearch\n",
    "> response is not (only) intended for humans. It is supposed to be used \n",
    "> by applications that run on the client machines, and therefore the\n",
    "> interface is called an Application Programming Interface (API). The \n",
    "> API uses a format called JSON (JavaScript Object Notation), which \n",
    "> can be easily read by machines (and humans). The API sends its JSON\n",
    "> response using the same method as your web browser displays web\n",
    "> pages. This method is called HTTP (Hyper Text Transfer Protocol), \n",
    "> and it is the reason you can inspect the response in a normal web\n",
    "> browser. APIs that use HTTP are called RESTful interfaces. REST \n",
    "> stands for REpresentational State Transfer, arguably one of the\n",
    "> simplest ways to define an API.\n",
    "\n",
    "\n",
    "### Kibana, cURL, and more cURL \n",
    "\n",
    "You can interact with your Elasticsearch service in different ways. In this first assignment we will describe three ways. Later during the practical work we will use the Python Elasticsearch client.\n",
    "\n",
    "1. Using the Kibana Console\n",
    "2. Using cURL\n",
    "3. Using cURL from a Jupyter notebook (not recommended)\n",
    "\n",
    "#### Kibana\n",
    "Kibana provides a web interface to interact with your Elasticsearch service. It's available from http://localhost:5601. You can use Kibana to create interactive dashboards visualizing data in your Elasticsearch indices. It also provides a console to execute Elasticsearch commands. It's available from http://localhost:5601/app/kibana#/dev_tools\n",
    "\n",
    "To start Kibana on the virtual machine, you can type `sudo service kibana start` in a Terminal.\n",
    "To stop the Kibana server, instead, you can type `sudo service kibana stop`.\n",
    "\n",
    "Many examples from the Elasticsearch user guide can be directly executed in Kibana by clicking the `VIEW IN CONSOLE` button.\n",
    "\n",
    "#### cURL\n",
    "[CURL](https://en.wikipedia.org/wiki/CURL) is a software tool that enables you to execute HTTP method requests from the commandline. The name originally stood for \"see URL\". \n",
    "\n",
    "Curl is already installed in the VM operating system. Let's open a bash terminal.\n",
    "You can exit the shell by executing `exit`.\n",
    "You can execute curl commands on this prompt, for instance retrieving the Elasticsearch state.\n",
    "Note you have to use `localhost` as the hostname:\n",
    "```\n",
    "labs@fir-machine:~$ curl localhost:9200\n",
    "{\n",
    "  \"name\" : \"epRATWu\",\n",
    "  \"cluster_name\" : \"docker-cluster\",\n",
    "  \"cluster_uuid\" : \"KsOTBsyeTmy6fJCcZ64d_A\",\n",
    "  \"version\" : {\n",
    "    \"number\" : \"6.2.4\",\n",
    "    \"build_hash\" : \"ccec39f\",\n",
    "    \"build_date\" : \"2018-04-12T20:37:28.497551Z\",\n",
    "    \"build_snapshot\" : false,\n",
    "    \"lucene_version\" : \"7.2.1\",\n",
    "    \"minimum_wire_compatibility_version\" : \"5.6.0\",\n",
    "    \"minimum_index_compatibility_version\" : \"5.0.0\"\n",
    "  },\n",
    "  \"tagline\" : \"You Know, for Search\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### cURL from this notebook\n",
    "\n",
    "Alternatively, jupyter notebooks allow you to directly execute cURL commands (or other shell commands), by starting a line of code with an exclamation mark (see example below). Plase be warned: when executing commands which result in long output (for instance when indexing a large number of documents), stick to the terminal to execute curl commands. Jupyter might freeze when handling long output from the shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Part 02 (Let's go!)\n",
    "\n",
    "_You can work on this part after Lecture 01 already, if you want_\n",
    "\n",
    "For the following exercises, you will use a TREC genomics document collection and queries. \n",
    "It is stored in the folder `data/` in the directory where you have been instructed to place the assignment notebooks (`/`).\n",
    "\n",
    "The collections contains:\n",
    "\n",
    "* `trec-medline.json` (the collection in Elasticsearch batch format - because of its size it cannot be indexed with a single curl command!)\n",
    "* `training-queries-simple.txt` (test queries)\n",
    "* `training-qrels.txt` (the \"relevance judgements\" for the test queries, i.e. the correct answers)\n",
    "* `test-queries-simple.txt`\n",
    "* `example_matches20.txt` (20 example matches)\n",
    "\n",
    "To make things easy, the data is already provided in Elasticsearch' batch processing format. \n",
    "Inspect the collection file in the terminal:\n",
    "\n",
    "`head trec-medline.json`\n",
    "\n",
    "This shows the first 5 documents in the collection (in JSON format prepared for ElasticSearch, as you have seen in the tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 02.A: _indexing_ and _first queries_\n",
    "\n",
    "Execute the following cell to index the collection in an Elasticsearch index called `genomics'. This code uses the Elasticsearch python api, which we will discuss later (you can read about it yourself, in the meanwhile).\n",
    "\n",
    "_Note:_ indexing the TREC genomics collection will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "import elasticsearch.helpers\n",
    "import json\n",
    "\n",
    "def documents():\n",
    "    \"\"\" generates the documents to be indexed as dictionaries \"\"\"\n",
    "    with open('data/trec-medline.json') as inp:\n",
    "        while True:\n",
    "            try:\n",
    "                line = next(inp)  # ignore odd line nrs\n",
    "                if line is None:\n",
    "                    break\n",
    "                try:\n",
    "                    docline = next(inp)\n",
    "                    doc = json.loads(docline)\n",
    "                    yield doc\n",
    "                except json.JSONDecodeError as e:\n",
    "                    # should not occur (but ignore it anyway)\n",
    "                    pass\n",
    "            except StopIteration as e:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = elasticsearch.Elasticsearch('localhost')\n",
    "\n",
    "# remove if it already exists\n",
    "es.indices.delete(index=\"genomics\", ignore=[400, 404])\n",
    "\n",
    "# and bulk index it\n",
    "print(\"Indexing documents, this will take some time...\")\n",
    "_ = elasticsearch.helpers.bulk(\n",
    "        es, \n",
    "        documents(),\n",
    "        index=\"genomics\",\n",
    "        chunk_size=2000,\n",
    "        request_timeout=30\n",
    "    )\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the index called Genomics and determine how many items are index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "# write the code here\n",
    "# BEGIN ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the command line (or the Kibana console), search for all documents that contain the word `blood`. \n",
    "\n",
    "How many documents containing the term `blood` are there in your index? (searching all fields of the documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "# write the code that generates the answer here (you may also use Kibana)\n",
    "# BEGIN ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 02.B: the Python ElasticSearch library\n",
    "\n",
    "#### Preparation\n",
    "The command line is fine for doing basic operations on your Elasticsearch indices, but as soon as things get more complex, you better use custom client programs.\n",
    "We will use the [Elasticsearch client library for Python](https://elasticsearch-py.readthedocs.io). This library will execute the HTTP requests that you have used before (with CURL or Kibana). The library is pre-installed on the VM.\n",
    "\n",
    "#### Exercise\n",
    "Write the code that searches the index for _\"blood\"_ using the [search()](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.Elasticsearch.search) function. Your code will take at minimum the following steps:\n",
    "\n",
    "1. import the python library `elasticsearch`.\n",
    "2. open a connection with the Elasticsearch host `'elasticsearch'` with `Elasticsearch()`.\n",
    "3. execute a search with `search()` using the index `genomics`, and a correct query body.\n",
    "4. print the JSON output of Elasticsearch \n",
    "\n",
    "How many hits are there in your index? Is the result the same as in Exercise 01.?\n",
    "\n",
    "> Elasticsearch runs on localhost on your laptop, at port 9200 (so as http://localhost:9200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "import elasticsearch\n",
    "\n",
    "# your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python client library returns Python objects, that use [dictionaries](https://docs.python.org/3.6/tutorial/datastructures.html#dictionaries) and [lists](https://docs.python.org/3.6/tutorial/introduction.html#lists).\n",
    "Use a [for loop](https://docs.python.org/3.6/tutorial/controlflow.html#for-statements) to inspect each hit, and print the retrieved document's titles one by one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "# your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 02.C: _Search using the Elasticsearch DSL_\n",
    "\n",
    "You will notice that the native query format of Elasticsearch can be quite verbose.\n",
    "Elasticsearh provides the Python library `elasticsearch_dsl` to write more concise Elasticsearch queries. \n",
    "This is only to simplify the syntax: the library still issues Elasticsearch queries.\n",
    "\n",
    "For example, a simple `match_all` query looks as follows:\n",
    "```python\n",
    "query = {\n",
    "   \"query\": {\n",
    "       \"match_all\": {}\n",
    "   }\n",
    "}\n",
    "```\n",
    "\n",
    "The same query can be created with the DSL as follows:\n",
    "```python\n",
    "query = Q(\"match_all\")\n",
    "```\n",
    "\n",
    "Especially for more complicated boolean queries, to use the native query format can become complicated.\n",
    "Read more about the DSL [here](https://elasticsearch-dsl.readthedocs.io/en/latest/search_dsl.html)\n",
    "\n",
    "__Exercise:__ Search for the query `blood` and check whether you get the same number of results as for exercise 02.B. You can use DSL, curl or Kibana. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "# your code here\n",
    "import elasticsearch\n",
    "# BEGIN ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making your own TREC run\n",
    "\n",
    "We will adopt a scientific approach to building search engines. That is, we are not only going to build a search engine and see that it works, but we are also going to _measure_ how well it works, by measuring the search engine's quality. We will adopt the method from the [Text Retrieval Conference](http://trec.nist.gov) (TREC). TREC provides researchers with test collections, that consists of 3 parts:\n",
    "\n",
    "1. the document collection (in our case a part of the MEDLINE database)\n",
    "2. the topics (which are natural language descriptions of what the user is searching for: you can think of the as the _queries_)\n",
    "3. the relevance judgments (for each topic, what documents are relevant)\n",
    "\n",
    "##  Exercise 02.D\n",
    "\n",
    "Complete the code of the Python function `make_trec_run()` that reads the topics [training-queries-simple.txt](http:training-queries-simple.txt), and for each topic does a search using Elasticsearch. The program should output a file in the [TREC submission format](https://trec-core.github.io/2017/#submission-guidelines). We already provided the first  lines for this exercise, which include:\n",
    "\n",
    "1. Open the file `'run_file_name'`' for writing and call it `run_file`.\n",
    "2. Open the file `'topics_file_name'` for reading, call it `test_queries`.\n",
    "3. For each line in `test_queries`:\n",
    "4. Remove the newline using `strip()`, then split the string on the tab character (`'\\t'`). The first part of the line is now `qid` (the query identifier) and the last part is `query` (a textual description of the query).\n",
    "5. complete the Python program such that the correct TREC run file is written to `'run_file_name'`.\n",
    "\n",
    "> **Note**: Make sure you output the `PMID` (pubmed identifier) of the document `hit['_source']['PMID']`. Do **not** use the elasticsearch identifier `_id` because they do not match the document identifiers in the relevance judgements. They were randomly generated by Elasticsearch during indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def make_trec_run(es, topics_file_name, run_file_name, run_name=\"test\"):\n",
    "    with open(run_file_name, 'w') as run_file:\n",
    "        with open(topics_file_name, 'r') as test_queries:\n",
    "            for line in test_queries:\n",
    "                (qid, query) = line.strip().split('\\t')\n",
    "                # BEGIN ANSWER\n",
    "                # END ANSWER\n",
    "                \n",
    "                \n",
    "# Write the results of the queries contained in the topic file `'data/training-queries-simple.txt'` \n",
    "# to the run file `'baseline.run'`, and name this test as `test01`\n",
    "make_trec_run(es, 'data/training-queries-simple.txt', 'baseline.run', run_name='test01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this prints out (it is a shell command) the content of the file baseline.run \n",
    "!cat baseline.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: Write a line to `run_file` using `run_file.write(line)`. \n",
    "> The newline character is: `'\\n'`. Before writing a number to\n",
    "> the file, cast it to a string using `str()`.\n",
    ">\n",
    "> The TREC Submission guidelines allow you to submit up to 1000\n",
    "> documents per topic. Keep this in mind!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index improvements: Tokenization (Analyzers)\n",
    "\n",
    "_You are advised to work on this part after Lecture 02_\n",
    "\n",
    "The way documents are indexed influences the performance of the IR systems. \n",
    "Elasticsearch [Mappings](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/mapping.html) define how a document, and its properties (fields) are stored and indexed. When using a different configuration of an ElasticSearch Mapping, the document collection needs to be re-indexed.\n",
    "\n",
    "## Background\n",
    "The following part of the assignment requires some self-study of the ElasticSearch tools to support the improvemnet of the indexing. Please read the:\n",
    "* [Index Settings and Mappings](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/indices-create-index.html).\n",
    "* Elasticsearch [Analyzers](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/analysis.html) contain many options for improving your search engine.\n",
    "\n",
    "You are again requested to use the [Python Elasticsearch Client](https://elasticsearch-py.readthedocs.io) library documentation.\n",
    "\n",
    "\n",
    "## Bulk indexing revisited\n",
    "As we need to re-index the document collection when we use a different Mapping configuration, we developed some functions to support a quick re-indexing in the following exercises.\n",
    "\n",
    "Below you find the Python code for bulk-indexing our Medline collection, similar to the code of the exercises in the beginning of Part 02. Read the code carefully, as you are required to use the indexing functions later for the completion of the assignment.\n",
    "\n",
    "> The code uses additional helper functions \n",
    "> (`elasticsearch.helpers`) and a library for processing JSON.\n",
    "> The function `read_documents()` reads the bulk insert file: The \n",
    "> function is a generator function. It generates an 'on-demand' list\n",
    "> by using the statement `yield` for every item of the list. It\n",
    "> is used in the helper function `elasticsearch.helpers.bulk()`.\n",
    "> The statement `raise` is Python's approach to throw exceptions, \n",
    "> that is, it exits the program with an error.\n",
    "> Note the additional (keyword) arguments to bulk:\n",
    "> `chunk_size` indicates the number of documents to be processed by\n",
    "> elasticsearch in one batch. \n",
    "> The request_timeout is set to 30 seconds because processing a single batch\n",
    "> of documents can take some time.\n",
    "\n",
    "> __Note:__ _when processing a bulk index, be sure to have few GigaBytes free on your hard drive. If you get a BulkIndexError with read-only/FORBIDDEN errors, you probably have too little hard drive space available for ElasticSearch to work properly._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "import elasticsearch.helpers\n",
    "import json\n",
    "\n",
    "es = elasticsearch.Elasticsearch(host='localhost')  # in case you use Docker, the host is 'elasticsearch'\n",
    "\n",
    "def read_documents(file_name):\n",
    "    \"\"\"\n",
    "    Returns a generator of documents to be indexed by elastic, read from file_name\n",
    "    \"\"\"\n",
    "    with open(file_name, 'r') as documents:\n",
    "        for line in documents:\n",
    "            doc_line = json.loads(line)\n",
    "            if ('index' in doc_line):\n",
    "                id = doc_line['index']['_id']\n",
    "            elif ('PMID' in doc_line):\n",
    "                doc_line['_id'] = id\n",
    "                yield doc_line\n",
    "            else:\n",
    "                raise ValueError('Woops, error in index file')\n",
    "\n",
    "def create_index(es, index_name, body={}):\n",
    "    # delete index when it already exists\n",
    "    es.indices.delete(index=index_name, ignore=[400, 404])\n",
    "    # create the index \n",
    "    es.indices.create(index=index_name, body=body)\n",
    "                \n",
    "def index_documents(es, file_name, index_name, body={}):\n",
    "    create_index(es, index_name, body)\n",
    "    # bulk index the documents from file_name\n",
    "    return elasticsearch.helpers.bulk(\n",
    "        es, \n",
    "        read_documents(file_name),\n",
    "        index=index_name,\n",
    "        chunk_size=2000,\n",
    "        request_timeout=30\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "ConnectionError(<urllib3.connection.HTTPConnection object at 0x7f3a543b0ca0>: Failed to establish a new connection: [Errno 111] Connection refused) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7f3a543b0ca0>: Failed to establish a new connection: [Errno 111] Connection refused)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             conn = connection.create_connection(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/elasticsearch/connection/http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             response = self.pool.urlopen(\n\u001b[0m\u001b[1;32m    246\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRetry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    720\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;31m# Disabled, indicate to re-raise the error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    702\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    666\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1285\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             raise NewConnectionError(\n\u001b[0m\u001b[1;32m    169\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f3a543b0ca0>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1841d6eb416b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/trec-medline.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'genomics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-1e8c51a30b93>\u001b[0m in \u001b[0;36mindex_documents\u001b[0;34m(es, file_name, index_name, body)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mindex_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mcreate_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;31m# bulk index the documents from file_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     return elasticsearch.helpers.bulk(\n",
      "\u001b[0;32m<ipython-input-7-1e8c51a30b93>\u001b[0m in \u001b[0;36mcreate_index\u001b[0;34m(es, index_name, body)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# delete index when it already exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m# create the index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/elasticsearch/client/utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/elasticsearch/client/indices.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, index, params, headers)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty value passed for a required argument 'index'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         return self.transport.perform_request(\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0;34m\"DELETE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_make_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/elasticsearch/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    388\u001b[0m                     \u001b[0;31m# raise exception on last retry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/elasticsearch/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m                 status, headers_response, data = connection.perform_request(\n\u001b[0m\u001b[1;32m    359\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/elasticsearch/connection/http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mConnectionTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TIMEOUT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"N/A\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# raise warnings if any from the 'Warnings' header.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ConnectionError(<urllib3.connection.HTTPConnection object at 0x7f3a543b0ca0>: Failed to establish a new connection: [Errno 111] Connection refused) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7f3a543b0ca0>: Failed to establish a new connection: [Errno 111] Connection refused)"
     ]
    }
   ],
   "source": [
    "index_documents(es, 'data/trec-medline.json', 'genomics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 02.E: _ElasticSearch Analyzers (Tokenization)_\n",
    "\n",
    "The amount and quality of the tokens used to construct the inverted index are of great importance. In ElasticSearch, mappings and settings also allow specifying what [Analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html) is used to tokenize your documents and queries. In the mappings below, use the _Dutch_ analyzer for the field `\"all\"` (where `\"all'` indexes the fields `\"TI\"` and `\"AB\"`):\n",
    "\n",
    "> Usually, the same analyzer should be applied to documents and queries, but \n",
    "> Elasticsearch allows you to specify a `\"search_analyzer\"` that is used on \n",
    "> your queries (which we do not need to use in the assignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "analyzer_test = {\n",
    "  # BEGIN ANSWER\n",
    "  # END ANSWER\n",
    "}\n",
    "\n",
    "# create the index, but don't index any documents:\n",
    "create_index(es, 'genomics', body=analyzer_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analyzer defined for the `\"all\"` field can be tested [as follows](https://elasticsearch-py.readthedocs.io/en/master/api.html#indices). Translated to English the text says: _\"This is a Dutch sentence\"_. \n",
    "\n",
    "    The following script identifies the tokens (based on the use of the dutch tokenizer): try with different tokenizers and different sentences to see how the tokens are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint # pretty print\n",
    "\n",
    "body = { \"field\": \"all\", \"text\": \"dit zijn nederlandse zinnen\"}\n",
    "tokens = es.indices.analyze(index='genomics', body=body)\n",
    "pprint(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercise 02.F: _tweet language analyzer_\n",
    "\n",
    "Read the documentation for [Custom Analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/analysis-custom-analyzer.html). \n",
    "Make a custom analyzer for _English tweet language_. The analyzer should do the following:\n",
    "* change common abbreviations to the full forms: \n",
    "  * _b4_ to _before_, \n",
    "  * _abt_ to _about_, \n",
    "  * _chk_ to _check_, \n",
    "  * _cr8_ to _create_, \n",
    "  * _dm_ to _direct message_,\n",
    "  * _f2f_ to _face-to-face_\n",
    "* use the _standard_ tokenizer;\n",
    "* put everything to lower-case;\n",
    "* filter English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "tweet_analyzer = {\n",
    "  # BEGIN ANSWER\n",
    "  # END ANSWER\n",
    "}\n",
    "\n",
    "# create the index, but don't index any documents:\n",
    "create_index(es, 'genomics', body=tweet_analyzer)\n",
    "body = { \"field\": \"all\", \"text\": \"cr8 it! what abt dm me?\"}\n",
    "tokens = es.indices.analyze(index='genomics', body=body)\n",
    "pprint(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 03: Search models \n",
    "\n",
    "_You are advised to work on this part after Lecture 03_\n",
    "\n",
    "Elasticsearch [Mappings](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/mapping.html) define how a document, and its properties (fields) are stored and indexed, but also provides tools to implement and exeute different document similarity measures (i.e. search models). \n",
    "\n",
    "> See again: [Index Settings and Mappings](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/indices-create-index.html).\n",
    "\n",
    "For instance, we can add a new field `\"all\"` that uses the  [similarity measure](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/similarity.html) _Boolean_, and let it serve as an index for the fields `\"TI\"` and `\"AB\"` (title and abstract) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean = {\n",
    "  \"settings\" : {\n",
    "    # a single shard, so we do not suffer from approximate document frequencies\n",
    "    \"number_of_shards\" : 1\n",
    "  },\n",
    "  \"mappings\": {\n",
    "      \"properties\": {\n",
    "        \"AB\": {\n",
    "          \"type\": \"text\",\n",
    "          \"similarity\": \"boolean\"\n",
    "        },\n",
    "        \"TI\": {\n",
    "          \"type\": \"text\",\n",
    "          \"similarity\": \"boolean\"\n",
    "        }\n",
    "      }\n",
    "  }\n",
    "}\n",
    "\n",
    "index_documents(es, 'data/trec-medline.json', 'genomics', body=boolean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Most changes to the mappings cannot be done on an existing index. Some (for instance\n",
    "> similarity measures) can be changed if the index is first closed. Nevertheless, we \n",
    "> will in this notebook _re-index_ the collection for every change to the mappings\n",
    "> using the function `index_documents()` that we defined above. Mappings (and settings)\n",
    "> can be passed to the function using the `body` parameter.\n",
    "\n",
    "Let's have a look at the mappings and settings for our index as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.get(index='genomics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's search our new field `\"all\"` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"blood\"\n",
    "search_type = \"dfs_query_then_fetch\" # this will use exact document frequencies even for multiple shards\n",
    "body = {\n",
    "  \"query\": {\n",
    "    \"match\" : { \"all\" : query }\n",
    "  },\n",
    "  \"size\": 10\n",
    "}\n",
    "es.search(index=\"genomics\", search_type=search_type, body=body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 03.A: _new run and evaluation_\n",
    "Create a new run file (e.g. `boolean.run`), compute the retrieval performance with the function `print_trec_eval()` and compare the results with the baseline run file `baseline.run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "# write your code here\n",
    "# BEGIN ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 03.B: _Language models_\n",
    "\n",
    "Custom similarities can be configured by tuning the parameters of the built-in similarities. Read more about these (expert) options in the [similarity module](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/index-modules-similarity.html).\n",
    "\n",
    "> Tip: the example similarity settings have to be used in a `\"settings\"` object.\n",
    "> Check your settings and mappings with: `es.indices.get(index='genomics')`.\n",
    "\n",
    "Make a run that uses Language Models with Jelinek-Mercer smoothing (linear interpolation smoothing) on the field `\"all\"` that indexes the fields `\"TI\"` and `\"AB\"`. Use the parameter `lambda=0.2`. Again evaluate the run using `print_trec_eval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "lmjelinekmercer = {\n",
    "  # BEGIN ANSWER\n",
    "  # END ANSWER\n",
    "}\n",
    "\n",
    "index_documents(es, 'data/trec-medline.json', 'genomics', body=lmjelinekmercer)\n",
    "make_trec_run(es, 'data/training-queries-simple.txt', 'lmjelinekmercer.run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 03.C: _Model comparison_\n",
    "Compute the results of the `lmjelinekmercer.run` and compare them with those of the `baseline.run` and `boolean.run`. Performing statistical tests may help strenghtening your claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "# your comments here\n",
    "# BEGIN ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -20 baseline.run\n",
    "! echo \"\\n\"\n",
    "! head -20 lmjelinekmercer.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 03.D: _Implement your own similarity measure (Bonus)_ \n",
    "\n",
    "We have only seen the results of using the analyzer to queries. The analyzer results from the _documents_ are available using the `termvectors()` function, as follows for document `id=1`: (Additionally, we can get overall field statistics, such as the number of documents)\n",
    "\n",
    "> First, index the collection again. While waiting, have a coffee or tea :) \n",
    "\n",
    "> `id=1` refers to the internal document identifiers, so not to the Pubmed identifier.\n",
    "\n",
    "_The bonus exercise is not mandatory. It can compensate for lower grades of other exercises._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_documents(es, 'data/trec-medline.json', 'genomics')\n",
    "\n",
    "es.termvectors(index=\"genomics\", id=\"1\", fields=\"TI\", \n",
    "               term_statistics=True, field_statistics=True, offsets=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the BM25 similarity\n",
    "\n",
    "Complete the function `bm25_similarity()` below by implementing the BM25 similarity as described by in Section 11.4.3 of [Manning, Raghavan and Schuetze, Chapter 11](https://nlp.stanford.edu/IR-book/pdf/11prob.pdf). Are you able to replicate the score of ElasitcSearch (15.472)? If not, are you using a different variant of the BM25 model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "import math\n",
    "\n",
    "# math.log(x) computes the logarithm of x\n",
    "\n",
    "def bm25_similarity (query, doc_id):\n",
    "\n",
    "    # Get the query tokens (see above)\n",
    "    query_tokens = es.indices.analyze(index='genomics', body={\"field\":\"TI\", \"text\": query})\n",
    "    tokens = query_tokens['tokens']\n",
    "\n",
    "    # Get the term vector for doc_id and the field statistics\n",
    "    term_vector = es.termvectors(index=\"genomics\", id=doc_id, fields=\"TI\", \n",
    "                  term_statistics=True, field_statistics=True, offsets=False)\n",
    "    vector = term_vector['term_vectors']['TI']['terms']\n",
    "    f_stats = term_vector['term_vectors']['TI']['field_statistics']\n",
    "\n",
    "    # The answer should sum over 'tokens', check if the tokens exists in the 'vector',\n",
    "    # and if so, add the appropriate value to 'similarity'.\n",
    "    # Tip: add print statements to your code to see what each variable contains.\n",
    "    \n",
    "    similarity = 0\n",
    "\n",
    "    # BEGIN ANSWER\n",
    "    # END ANSWER\n",
    "    return similarity\n",
    "\n",
    "bm25_similarity(\"curve fitting\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below for the 'reference score' computed by ElasticSearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "  \"query\": {\n",
    "    \"match\" : { \"TI\" : \"curve fitting\" }\n",
    "  }\n",
    "}\n",
    "explain = es.explain(index=\"genomics\", id=\"1\", body=body)\n",
    "print (explain['explanation']['value'])  # BM25 score computed by ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
